# Digital-Blanket-Batch-2-Project

# Energy Optimization Model

This repository contains an energy optimization model using reinforcement learning to regulate air conditioning based on room temperature, outside temperature, and energy usage.

## Prerequisites
Make sure you have the following installed:
- Python 3.8+
- pip
- Virtual Environment (optional but recommended)
- Required Python packages

## Installation

1. **Clone the Repository**
   ```sh
   git clone <repository-url>
   cd digital-blacket
   ```

2. **Create a Virtual Environment (Optional but Recommended)**
   ```sh
   python -m venv venv
   source venv/bin/activate  # On macOS/Linux
   venv\Scripts\activate  # On Windows
   ```

3. **Install Dependencies**
   ```sh
   pip install -r requirements.txt
   ```

## Running the Model

### 1Ô∏è‚É£ Train the Model
Run the training script to train the reinforcement learning model:
```sh
python model/train_model.py
```
Once training is complete, the model will be saved as:
```sh
model/energy_optimizer.zip
```

### 2Ô∏è‚É£ Test the Model
After training, test the model with:
```sh
python model/test_model.py
```
This will output a series of test actions and rewards based on sample inputs.

### 3Ô∏è‚É£ Start the API Server
Run the Flask API to serve the model:
```sh
python app.py
```
This will start the API on `http://127.0.0.1:5000`.

### 4Ô∏è‚É£ Send Requests to the API
Use `Invoke-RestMethod` (PowerShell) or `curl` to send data to the model and get predictions:

#### PowerShell:
```powershell
Invoke-RestMethod -Uri "http://127.0.0.1:5000/optimize" -Method Post -Headers @{"Content-Type"="application/json"} -Body '{"room_temp": 24, "outside_temp": 30, "energy": 40}'
```

#### Curl (Linux/Mac/Windows WSL):
```sh
curl -X POST "http://127.0.0.1:5000/optimize" -H "Content-Type: application/json" -d '{"room_temp": 24, "outside_temp": 30, "energy": 40}'
```

## Expected API Response
```json
{
  "action": 0
}
```
The response contains an **action**:
- `0`: Increase AC power (cooling)
- `1`: Decrease AC power (heating)
- `2`: Maintain current settings

## Adjusting Model Parameters
- Modify `energy_env.py` for custom reward functions and state transitions.
- Tune hyperparameters in `train_model.py` to improve performance.

## Troubleshooting
- Ensure Flask is running before sending requests.
- Check dependencies with `pip freeze`.
- Review logs for training performance.

## License
This project is open-source. Modify and use it as needed!

## Author
- **Your Name**
- Contact: your-email@example.com


This repo is for the Batch 2 interns of Digital Blanket to create an Agentic AI microservices project
8. README (README.md)
md
Copy
Edit
# Energy Optimization Microservice

## Overview
This microservice optimizes energy usage in smart buildings using Reinforcement Learning (RL).

## Steps to Run
1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
Train the RL model
bash
Copy
Edit
python model/train_model.py
Run the API
bash
Copy
Edit
python api/server.py
Test API
bash
Copy
Edit
curl -X POST http://localhost:5000/optimize -H "Content-Type: application/json" -d '{"room_temp": 24, "outside_temp": 30, "energy": 40}'
Deployment
Docker:
bash
Copy
Edit
docker build -t energy-optimizer .
docker run -p 5000:5000 energy-optimizer
Kubernetes:
bash
Copy
Edit
kubectl apply -f deployment/deployment.yaml
markdown
Copy
Edit

---

### **Final Checklist ‚úÖ**
‚úî **Custom RL Environment** (`env/energy_env.py`)  
‚úî **Train RL Model** (`model/train_model.py`)  
‚úî **Test RL Model** (`model/test_model.py`)  
‚úî **Deploy as Flask API** (`api/server.py`)  
‚úî **Containerization with Docker** (`deployment/Dockerfile`)  
‚úî **Kubernetes Deployment** (`deployment/deployment.yaml`)  
‚úî **Complete Documentation** (`README.md`)  

üöÄ Now your **Optimization Microservice is production-ready!** üöÄ Let me know if you need help with 
üöÄ Now your **Optimization Microservice is production-ready!** üöÄ Let me know if you need help with deployment on AWS/GCP! 



‚úîÔ∏è Step 2: Connect to a Real Sensor API
If you have a real IoT device or another backend system, replace:

python
Copy
Edit
SENSOR_API_URL = "http://127.0.0.1:5000/sensor-data"
With the actual API URL of your sensor system.



üìå 1Ô∏è‚É£ How Does It Work Now? (Self-Generated Data)
train_model.py trains the RL model using simulated environment data (energy_env.py).

test_model.py tests the model using the same environment.

server.py takes manual input from API requests but does NOT fetch real-time data.

üîπ Current server.py Workflow
1Ô∏è‚É£ User sends a request with sensor values (manual input)
2Ô∏è‚É£ RL model predicts an action (0, 1, or 2)
3Ô∏è‚É£ API returns the action to the user

üîπ Example: How API is Used Now
bash
Copy
Edit
curl -X POST http://127.0.0.1:5000/optimize -H "Content-Type: application/json" -d '{"room_temp": 24, "outside_temp": 30, "energy": 40}'
‚úÖ API only works when a user manually sends data.

üìå 2Ô∏è‚É£ How to Connect API to Real-Time Data?
üîπ To make the RL model automatically use real-time data, modify energy_env.py to fetch live sensor values.

‚úîÔ∏è Solution: Modify energy_env.py to Fetch Data from an API
Instead of using a fixed state, fetch data dynamically from another API that provides real-time sensor values.

üîπ Modify energy_env.py:

python
Copy
Edit
import requests
import gymnasium as gym
from gymnasium import spaces
import numpy as np

# API endpoint for fetching real-time sensor data
API_URL = "http://127.0.0.1:5000/sensor-data"

def get_real_time_data():
    """Fetch real-time sensor data from an external API."""
    try:
        response = requests.get(API_URL)
        if response.status_code == 200:
            return response.json()  # Example response: {"room_temp": 24, "outside_temp": 30, "energy": 40}
    except:
        print("Error fetching real-time data. Using default values.")
    return {"room_temp": 22, "outside_temp": 25, "energy": 50}  # Default values

class EnergyOptimizationEnv(gym.Env):
    def __init__(self):
        super(EnergyOptimizationEnv, self).__init__()

        self.observation_space = spaces.Box(low=np.array([15, 0, 0]), high=np.array([30, 50, 100]), dtype=np.float32)
        self.action_space = spaces.Discrete(3)

        # Get real-time sensor data instead of using a fixed state
        data = get_real_time_data()
        self.state = np.array([data["room_temp"], data["outside_temp"], data["energy"]], dtype=np.float32)
‚úÖ Now, the RL model will get live sensor data from an external API!

üìå 3Ô∏è‚É£ How Does It Work After This Change?
1Ô∏è‚É£ The environment (energy_env.py) requests real-time sensor data
2Ô∏è‚É£ The RL model trains using actual sensor values instead of fixed values
3Ô∏è‚É£ The trained model is then used by server.py to make real-time predictions
4Ô∏è‚É£ No need for manual input anymore! The system runs automatically

üìå 4Ô∏è‚É£ Updating server.py to Fetch Real-Time Data
Now, modify server.py to always fetch real-time data before predicting.

üîπ Modify server.py:

python
Copy
Edit
from flask import Flask, request, jsonify
from stable_baselines3 import PPO
import numpy as np
import requests

app = Flask(__name__)

# Load trained model
model = PPO.load("model/energy_optimizer.zip")

# API endpoint for real-time sensor data
SENSOR_API_URL = "http://127.0.0.1:5000/sensor-data"

def get_real_time_sensor_data():
    """Fetch live data from sensor API before predicting."""
    try:
        response = requests.get(SENSOR_API_URL)
        if response.status_code == 200:
            return response.json()  # Example response: {"room_temp": 24, "outside_temp": 30, "energy": 40}
    except:
        print("Error fetching real-time data. Using default values.")
    return {"room_temp": 22, "outside_temp": 25, "energy": 50}  # Default fallback values

@app.route('/optimize', methods=['GET'])
def optimize():
    """Fetch real-time sensor data and predict the best action."""
    data = get_real_time_sensor_data()
    state = np.array([data["room_temp"], data["outside_temp"], data["energy"]], dtype=np.float32)
    
    # Predict the best action using the RL model
    action, _ = model.predict(state)

    return jsonify({"action": int(action)})

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5000)
‚úÖ Now, whenever /optimize is called, it will fetch live sensor data instead of requiring manual input!

üìå 5Ô∏è‚É£ What Happens After These Changes?
Step	Before (Manual Input Required)	After (Real-Time Data Automatically Used)
üì° Data Collection	User manually sends input via API	System automatically fetches sensor data
üéØ RL Model Training	Uses self-generated environment	Uses real-time sensor values
üöÄ API Prediction	Needs user input to predict	Fetches data & predicts automatically
üîÑ Automation	Requires manual requests	Runs continuously without input
‚úÖ Now, the RL model works in real-time with live sensor data!

üìå 6Ô∏è‚É£ What to Do Next?
‚úîÔ∏è Step 1: Test the Changes Locally
1Ô∏è‚É£ Start the modified API

bash
Copy
Edit
python api/server.py
2Ô∏è‚É£ Test if it automatically fetches sensor data

bash
Copy
Edit
curl -X GET http://127.0.0.1:5000/optimize
üîπ Expected output (automatically predicted action):

json
Copy
Edit
{
    "action": 1
}
